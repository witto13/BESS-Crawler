# Performance-Analyse: Warum dauert der Crawl so lange?

## üîç Hauptursachen

### 1. **Rate-Limiting (Compliance)**
- **Problem**: 1 Sekunde Delay zwischen **jedem Request**
- **Auswirkung**: 
  - Bei 4.317 Procedures √ó 1s = **~72 Minuten nur f√ºr Delays**
  - Plus Dokument-Downloads (jede Procedure hat mehrere Dokumente)
- **Warum**: Compliance mit robots.txt und AGBs

### 2. **Dokument-Downloads**
- **Problem**: Jede Procedure l√§dt alle Dokumente herunter
- **Auswirkung**:
  - DiPlanung: ~3-5 Dokumente pro Procedure
  - 4.317 Procedures √ó 3 Dokumente √ó 1s Delay = **~3,6 Stunden nur f√ºr Dokumente**
- **Warum**: F√ºr Extraktionen (MW, Hektar, Datum, Firmen) ben√∂tigt

### 3. **Geobasis-BB: 10 Sekunden Delay**
- **Problem**: robots.txt erfordert `crawl-delay: 10`
- **Auswirkung**: XPlanung-WFS Requests sind **10x langsamer**
- **Warum**: Compliance mit robots.txt

### 4. **Sequenzielle Verarbeitung**
- **Problem**: Procedures werden nacheinander verarbeitet
- **Auswirkung**: Keine Parallelisierung
- **Warum**: Einfacheres Error-Handling, aber langsamer

### 5. **Viele kleine Jobs**
- **Problem**: 422 Jobs in Queue (RIS/Gazette)
- **Auswirkung**: Jeder Job macht nur wenige Requests, aber mit Delays
- **Warum**: Pro Gemeinde ein Job

## üìä Aktuelle Zahlen

```
Total Procedures: 4.317
Queue: 422 Jobs
Rate: ~1 Procedure/Sekunde (mit Delays)
Gesch√§tzte Zeit: ~1-2 Stunden f√ºr verbleibende Jobs
```

## ‚ö° Beschleunigungs-Optionen

### Option 1: Rate-Limiting reduzieren (‚ö†Ô∏è Risiko)
- **Aktuell**: 1 Sekunde
- **Vorschlag**: 0.5 Sekunden (nur f√ºr nicht-kritische Domains)
- **Risiko**: K√∂nnte gegen robots.txt versto√üen
- **Gewinn**: ~50% schneller

### Option 2: Parallelisierung
- **Aktuell**: 1 Worker, sequenziell
- **Vorschlag**: Mehrere Worker parallel
- **Risiko**: H√∂here Server-Last
- **Gewinn**: 2-4x schneller (je nach Worker-Anzahl)

### Option 3: Dokument-Downloads optional
- **Aktuell**: Alle Dokumente werden heruntergeladen
- **Vorschlag**: Nur bei High-Confidence oder auf Anfrage
- **Risiko**: Weniger Extraktionen
- **Gewinn**: ~70% schneller

### Option 4: Batch-Processing
- **Aktuell**: Ein Request pro Procedure
- **Vorschlag**: Mehrere Procedures pro Request (wenn API unterst√ºtzt)
- **Risiko**: Nicht alle APIs unterst√ºtzen das
- **Gewinn**: 2-5x schneller

## üéØ Empfohlene L√∂sung

### Kurzfristig (Schnell)
1. **Mehrere Worker**: 2-3 Worker parallel
2. **Dokument-Downloads optimieren**: Nur bei High-Score

### Langfristig (Optimal)
1. **Intelligentes Rate-Limiting**: Domain-spezifisch
2. **Parallelisierung**: Mehrere Worker + Threading
3. **Caching**: Dokumente nicht doppelt herunterladen

## ‚è±Ô∏è Gesch√§tzte Zeit

### Aktuell (mit Compliance)
- **DiPlanung**: ~1-2 Stunden (mit Dokumenten)
- **RIS/Gazette**: ~30-60 Minuten (422 Jobs √ó ~5s pro Job)
- **XPlanung**: ~10-20 Minuten (mit 10s Delay)
- **Gesamt**: ~2-4 Stunden

### Mit Optimierungen
- **Mit 3 Workern**: ~40-80 Minuten
- **Ohne Dokument-Downloads**: ~20-40 Minuten
- **Kombiniert**: ~15-30 Minuten

## üí° Empfehlung

**F√ºr jetzt**: Crawl laufen lassen (ist fast fertig - 422 Jobs = ~30-60 Minuten)

**F√ºr zuk√ºnftige Crawls**: 
1. Mehrere Worker aktivieren
2. Dokument-Downloads optimieren (nur bei High-Score)
3. Intelligentes Rate-Limiting






